# app.py
# PO PDF â†’ CSV ç»Ÿä¸€è§£æä¸å¯¼å‡ºï¼ˆé€‚é…ä¸‰å®¶ä¾›åº”å•† + åŒå¯¼å…¥æ¨¡ç‰ˆï¼‰
# - è§£æï¼šCamelot/Tabula/pdfplumber å¤šçº§å›é€€ + æ­£åˆ™é€è¡Œ + å¯é€‰ OCR
# - ä¾›åº”å•†ï¼šFoodstuffs_NI / MyFoodBag / WWNZ(Countdown)
# - å¯¼å‡ºï¼šDCorder(å« price) ä¸ CDDCorder(ä¸å« price)
# - é—¨åº—æ˜ å°„ï¼šä¾§æ ä¸Šä¼  store_map.csvï¼ˆsite_code/name â†’ store_id/nameï¼‰

import re
import json
import tempfile
from pathlib import Path
from datetime import datetime

import streamlit as st
import pandas as pd

# å¯é€‰ä¾èµ–ï¼ˆå®‰è£…åå¯å–æ¶ˆæ³¨é‡Šç›¸åº”å¯¼å…¥ï¼‰
# import camelot
# from tabula import read_pdf
# import pdfplumber
# from pdf2image import convert_from_path
# import pytesseract
# from PIL import Image

# -----------------------
# åŸºç¡€è®¾ç½®
# -----------------------
st.set_page_config(page_title="PO PDF â†’ CSV Converter", layout="wide")
st.title("PO PDF â†’ CSV è½¬æ¢å™¨ï¼ˆé€‚é…ä¸‰å®¶ä¾›åº”å•† & åŒå¯¼å…¥æ¨¡ç‰ˆï¼‰")

# -----------------------
# ä¾›åº”å•†é…ç½®ï¼ˆå¯åœ¨ä¾§æ å¯¼å…¥/å¯¼å‡º JSONï¼‰
# -----------------------
VENDOR_PROFILES = {
    "Foodstuffs_NI": {
        "detect_keywords": ["Foodstuffs North Island Limited", "Order Forecast", "O/F"],
        "engine": "camelot-lattice",
        "pages": "all",
        "store_regex": r"(Delivery\s+To|Delivery\s+Address)[:ï¼š]?\s*([A-Za-z0-9\-\&\(\)\/., ]+)",
        "column_map": {
            "Line": "Line",
            "Item #": "ItemNo",
            "Article Number": "Article",
            "Product Description": "Item",
            "Order Qty": "Qty",
            "Purchasing Unit of Measure": "UoM",
            "Units Per Purchasing UoM": "UnitsPerUoM",
            "Price Per Ord. Unit": "Unit Price",
            "Total Price": "Line Total"
        },
        "skip_total_rows": True,
        "header_extract": {
            "PO_Number": r"Order\s+Forecast\s+Number:\s*([0-9]+)",
            "Order_Date": r"Date\s+of\s+Order:\s*([0-9/]+)",
            "Delivery_Date": r"Delivery\s+Date:\s*([0-9/]+)"
        },
        "line_regex": r"""
          ^\s*(\d+)\s+                 # Line
          (\d+)\s+                     # Item #
          ([A-Za-z0-9\-]+)\s+          # Article
          (.+?)\s+                     # Product Description
          (\d+)\s+                     # Order Qty
          ([A-Z]{2,4})\s+              # UoM
          (\d+)\s+                     # Units Per UoM
          ([0-9]+\.[0-9]{2})\s+        # Unit Price
          ([0-9]+\.[0-9]{2})\s+        # Net (å¿½ç•¥)
          (-?[0-9]+\.[0-9]{2})\s+      # Term Fee (å¿½ç•¥)
          ([0-9]+\.[0-9]{2})\s*$       # Line Total
        """
    },
    "MyFoodBag": {
        "detect_keywords": ["My Food Bag Limited", "PURCHASE ORDER", "GST Reg. No:"],
        "engine": "camelot-stream",
        "pages": "all",
        "store_regex": r"(Delivery\s+Instructions:|My\s+Food\s+Bag)\s*([A-Za-z0-9\-\&\(\)\/., ]+)",
        "column_map": {
            "Item No.": "ItemNo",
            "QTY": "Qty",
            "DESCRIPTION": "Item",
            "Delivery Date": "Delivery_Date",
            "PRICE": "Unit Price",
            "TOTAL": "Line Total"
        },
        "skip_total_rows": True,
        "header_extract": {
            "PO_Number": r"Purchase\s+No:\s*([0-9]+)",
            "Order_Date": r"Order\s+Date\s*([0-9/]+)"
        },
        "line_regex": r"""
          ^\s*([0-9]{6,})\s+           # Item No
          ([0-9,]+)\s+                 # QTY
          (.+?)\s+                     # DESCRIPTION
          ([0-9/]{6,})\s+              # Delivery Date
          ([0-9]+\.[0-9]{2})\s+        # PRICE
          ([0-9,]+\.[0-9]{2})\s*$      # TOTAL
        """
    },
    "WWNZ": {
        "detect_keywords": [
            "WOOLWORTHS NZ", "NEW PURCHASE ORDER", "VENDOR COPY",
            "WOOLWORTHS NEW ZEALAND", "PRODUCE ORDER NUMBER"
        ],
        "engine": "camelot-stream",
        "pages": "all",
        "store_regex": r"Deliver\s+To:\s*([0-9]{3,6})\s*\n\s*([^\n]+)",
        "column_map": {
            "ITEM DESCRIPTION": "Item",
            "ITEM NO": "ItemNo",
            "ORD QTY": "Qty",
            "PRICE EXCL": "Unit Price"
        },
        "skip_total_rows": True,
        "header_extract": {
            "PO_Number": r"PRODUCE\s+ORDER\s+NUMBER\s*:\s*([0-9]+)",
            "Order_Date": r"Order\s+Date\s*:\s*([0-9/]{8,})",
            "Delivery_Date": r"Delivery\s+Date\s*:\s*([0-9/]{8,})",
            "Delivery_Time": r"Delivery\s+Time\s*:\s*([0-9\s:]{4,})"
        },
        "line_regex": r"""
          ^\s*\d+\s+                    # è¡Œå·
          (?P<gtin>\d{8,14})\s+         # GTIN
          (?P<item>.+?)\s+              # æè¿°
          (?P<itemno>\d+)\s+            # ITEM NO
          (?P<om>[\d\.]+)\s+            # OM (å¯å¿½ç•¥)
          (?P<tihi>\d+x\d+)\s+          # TIxHI (å¯å¿½ç•¥)
          (?P<ordqty>\d+)\s+            # ORD QTY
          (?P<taxes>\d+)\s+             # TAXES (å¯å¿½ç•¥)
          (?P<price>[\d]+\.[\d]{2})\s*$ # PRICE EXCL
        """
    }
}

# -----------------------
# å·¥å…·å‡½æ•°
# -----------------------
def clean_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    for c in df.columns:
        if df[c].dtype == object:
            df[c] = df[c].astype(str).str.strip()
    return df

def normalize_numeric(series):
    """Robust numeric cleaner:
    - Accepts Series/list-like/scalars
    - Unwraps list/tuple cells (takes first element)
    - Strips currency/thousand separators
    - Extracts first numeric token
    - Coerces invalids to NA
    """
    s = pd.Series(series)
    s = s.apply(lambda x: (x[0] if isinstance(x, (list, tuple)) and len(x) > 0 else x))
    s = s.astype(str)
    s = s.str.replace(r"[\s,\$]", "", regex=True)
    s = s.str.extract(r"(-?\d+(?:\.\d+)?)", expand=False)
    return pd.to_numeric(s, errors="coerce")

def pick_first_existing(df, candidates):
    for c in candidates:
        if c in df.columns:
            return c
    return None

def parse_date_safe(s: str):
    if not s:
        return None
    for fmt in ("%d/%m/%Y", "%d/%m/%y", "%Y/%m/%d", "%d-%m-%Y", "%Y-%m-%d"):
        try:
            return datetime.strptime(s.strip(), fmt).date().isoformat()
        except Exception:
            continue
    return s  # è§£æå¤±è´¥åˆ™ä¿ç•™åŸå€¼

def extract_by_regex(pattern, text):
    if not pattern or not text:
        return None
    m = re.search(pattern, text, flags=re.IGNORECASE)
    if m:
        if m.lastindex and m.lastindex >= 1:
            if m.lastindex == 1:
                return m.group(1).strip()
            return [g.strip() if isinstance(g, str) else g for g in m.groups()]
        return m.group(0).strip()
    return None

def extract_store(text, store_re):
    val = extract_by_regex(store_re, text)
    site_code, store_name = None, None
    if isinstance(val, list):
        if len(val) >= 2:
            if re.fullmatch(r"\d{3,6}", val[0]):
                site_code, store_name = val[0], val[1]
            else:
                store_name = val[1]
        elif len(val) == 1:
            store_name = val[0]
    elif isinstance(val, str):
        store_name = val
    return site_code, store_name

def detect_vendor(text_concat: str) -> str | None:
    text_upper = (text_concat or "").upper()
    for vendor, cfg in VENDOR_PROFILES.items():
        for kw in cfg.get("detect_keywords", []):
            if kw.upper() in text_upper:
                return vendor
    return None

# è§£æå¼•æ“ä»¬
def parse_with_pdfplumber(pdf_path, pages_spec):
    import pdfplumber
    dfs, raw_texts = [], []
    with pdfplumber.open(pdf_path) as pdf:
        idx = range(len(pdf.pages))
        if pages_spec != "all":
            wanted = set()
            for seg in pages_spec.split(","):
                seg = seg.strip()
                if "-" in seg:
                    a, b = seg.split("-")
                    wanted |= set(range(int(a)-1, int(b)))
                else:
                    wanted.add(int(seg)-1)
            idx = [i for i in idx if i in wanted]
        for i in idx:
            page = pdf.pages[i]
            raw_texts.append(page.extract_text() or "")
            table = page.extract_table()
            if table and len(table) > 1:
                df = pd.DataFrame(table[1:], columns=table[0])
                dfs.append(df)
    return dfs, "\n".join(raw_texts)

def parse_with_camelot(pdf_path, pages_spec, flavor):
    import camelot
    pages_arg = pages_spec if pages_spec != "all" else "all"
    tables = camelot.read_pdf(pdf_path, pages=pages_arg, flavor=flavor, strip_text="\n")
    dfs = []
    for t in tables:
        df = t.df
        if df.shape[0] > 1:
            new_df = df.copy()
            new_df.columns = new_df.iloc[0]
            new_df = new_df.iloc[1:].reset_index(drop=True)
            dfs.append(new_df)
    return dfs

def parse_with_tabula(pdf_path, pages_spec):
    from tabula import read_pdf
    pages_arg = pages_spec if pages_spec != "all" else "all"
    dfs = read_pdf(pdf_path, pages=pages_arg, multiple_tables=True, lattice=False) or []
    return dfs

def ocr_pages_to_text(pdf_path):
    from pdf2image import convert_from_path
    import pytesseract
    images = convert_from_path(pdf_path, dpi=200)
    texts = []
    for img in images:
        texts.append(pytesseract.image_to_string(img))
    return "\n".join(texts)

def coerce_to_standard(df, item_candidates, qty_candidates, price_candidates, fallback_store, id_candidates=None):
    df = clean_cols(df)
    item_col = pick_first_existing(df, item_candidates) or df.columns[0]
    qty_col = pick_first_existing(df, qty_candidates)
    price_col = pick_first_existing(df, price_candidates)

    # è‡ªåŠ¨çŒœæµ‹
    if qty_col is None:
        maybe = [c for c in df.columns if re.search(r"\b(qty|quantity|æ•°é‡)\b", str(c), re.I)]
        qty_col = maybe[0] if maybe else None
    if price_col is None:
        maybe = [c for c in df.columns if re.search(r"(unit\s*price|price|å•ä»·|ä»·æ ¼)", str(c), re.I)]
        price_col = maybe[0] if maybe else None

    out = pd.DataFrame()
    out["Item"] = df[item_col].astype(str)
    out["Qty"] = normalize_numeric(df[qty_col]) if (qty_col and qty_col in df.columns) else pd.NA
    out["Unit Price"] = normalize_numeric(df[price_col]) if (price_col and price_col in df.columns) else pd.NA
    if id_candidates is None:
        id_candidates = ["ItemNo","Item #","Item No.","ITEM NO","Article Number","Article","SKU","PLU","GTIN","UPC","Barcode"]
    id_col = pick_first_existing(df, id_candidates)
    out["ItemNo"] = df[id_col].astype(str) if id_col and id_col in df.columns else pd.NA
    out["Store"] = fallback_store
    return out

# å¯¼å‡ºæ¨¡ç‰ˆå·¥å…·
def map_store_to_ids(site_code, store_name, store_map_df):
    """æ ¹æ® site_code æˆ– store_name åœ¨æ˜ å°„è¡¨ä¸­æ‰¾åˆ° store_id å’Œ nameã€‚"""
    if store_map_df is None:
        return None, None

    df = store_map_df.copy()
    df.columns = [c.strip().lower() for c in df.columns]

    sid, sname = None, None
    # 1) site_code ç²¾ç¡®åŒ¹é…
    if site_code:
        m = df[df.get("site_code", "").astype(str).str.strip() == str(site_code).strip()]
        if not m.empty:
            sid = m.iloc[0].get("store_id")
            sname = m.iloc[0].get("name")
            return sid, sname
    # 2) store_name æ¨¡ç³ŠåŒ¹é…
    if store_name:
        key = str(store_name).strip().lower()
        m = df[df.get("name", "").astype(str).str.lower().str.contains(re.escape(key))]
        if not m.empty:
            sid = m.iloc[0].get("store_id")
            sname = m.iloc[0].get("name")
            return sid, sname
    return None, None

def to_DCorder_template(df):
    """å¯¼å‡ºä¸º DCorderï¼ˆå« priceï¼‰"""
    out = pd.DataFrame({
        "store_id": df.get("store_id"),
        "name": df.get("name"),
        "sales_id": df.get("PO_Number"),
        "order_date": df.get("Delivery_Date").fillna(df.get("Order_Date")),
        "item_id": df.get("ItemNo"),
        "quantity": df.get("Qty"),
        "price": df.get("Unit Price")
    })
    return out

def to_CDDCorder_template(df):
    """å¯¼å‡ºä¸º CDDCorderï¼ˆä¸å« priceï¼‰"""
    out = pd.DataFrame({
        "store_id": df.get("store_id"),
        "name": df.get("name"),
        "sales_id": df.get("PO_Number"),
        "order_date": df.get("Delivery_Date").fillna(df.get("Order_Date")),
        "item_id": df.get("ItemNo"),
        "quantity": df.get("Qty")
    })
    return out

def choose_format_by_vendor(vendor: str) -> str:
    """æŒ‰ä¾›åº”å•†è‡ªåŠ¨é€‰æ‹©å¯¼å‡ºæ ¼å¼ï¼šWWNZâ†’CDDCorderï¼Œå…¶å®ƒâ†’DCorder"""
    if (vendor or "").upper() == "WWNZ":
        return "CDDCorder"
    return "DCorder"

# ä¸»è§£æ
def try_parse(pdf_bytes, file_name, vendor_choice, enable_ocr, default_engine, default_pages, default_store_regex,
              item_cols_guess, qty_cols_guess, price_cols_guess):
    with tempfile.TemporaryDirectory() as td:
        fp = Path(td) / file_name
        fp.write_bytes(pdf_bytes)

        # å…ˆæçº¯æ–‡æœ¬ï¼ˆç”¨äºè¯†åˆ«ä¾›åº”å•†ä¸æŠ¬å¤´æŠ½å–ï¼‰
        text_concat = ""
        try:
            import pdfplumber
            with pdfplumber.open(str(fp)) as pdf:
                text_concat = "\n".join([p.extract_text() or "" for p in pdf.pages])
        except Exception:
            text_concat = ""

        # ä¾›åº”å•†é€‰æ‹©ä¸é…ç½®
        active_vendor = detect_vendor(text_concat) if vendor_choice == "Auto" else (vendor_choice if vendor_choice in VENDOR_PROFILES else None)
        profile = VENDOR_PROFILES.get(active_vendor, {}) if active_vendor else {}
        engine_used = profile.get("engine", default_engine)
        pages_used = profile.get("pages", default_pages)
        store_re_used = profile.get("store_regex", default_store_regex)
        colmap = profile.get("column_map", {})
        skip_total_rows = profile.get("skip_total_rows", True)
        header_extract = profile.get("header_extract", {})
        line_regex = profile.get("line_regex", "")

        # æŠ¬å¤´å­—æ®µæŠ½å–
        header = {}
        for k, pat in header_extract.items():
            v = extract_by_regex(pat, text_concat)
            if isinstance(v, list):
                v = " ".join([str(x) for x in v if x])
            header[k] = v

        # åº—é¢/ä»“ä¿¡æ¯
        site_code, store_name = extract_store(text_concat, store_re_used) if text_concat else (None, None)

        # è§£æè¡¨æ ¼ï¼šå¤šçº§å›é€€
        tables = []
        # 1) é¦–é€‰ Camelot
        try:
            if engine_used.startswith("camelot"):
                flavor = "lattice" if engine_used.endswith("lattice") else "stream"
                tables = parse_with_camelot(str(fp), pages_used, flavor)
        except Exception as e:
            st.warning(f"[{file_name}] Camelot è§£æå¤±è´¥ï¼š{e}")

        # 2) Tabula å›é€€
        if not tables:
            try:
                tables = parse_with_tabula(str(fp), pages_used)
            except Exception:
                pass

        # 3) pdfplumber å›é€€
        if not tables:
            try:
                tables, text2 = parse_with_pdfplumber(str(fp), pages_used)
                if text2 and (not text_concat):
                    text_concat = text2
            except Exception:
                pass

        # 4) OCR çº¯æ–‡æœ¬ï¼ˆå¼€å¯æ—¶ï¼‰
        if enable_ocr and not (text_concat or "").strip():
            try:
                text_concat = ocr_pages_to_text(str(fp))
            except Exception as e:
                st.error(f"[{file_name}] OCR å¤±è´¥ï¼š{e}")

        # æ„å»ºåˆ—å€™é€‰
        item_candidates = [k for k, v in colmap.items() if v == "Item"] or [c.strip() for c in item_cols_guess.split(",")]
        qty_candidates = [k for k, v in colmap.items() if v == "Qty"] or [c.strip() for c in qty_cols_guess.split(",")]
        price_candidates = [k for k, v in colmap.items() if v == "Unit Price"] or [c.strip() for c in price_cols_guess.split(",")]
        id_candidates = [k for k, v in colmap.items() if v in ("ItemNo", "ITEM NO")] or ["ItemNo","Item #","Item No.","ITEM NO","Article Number","Article","SKU","PLU","GTIN","UPC","Barcode"]

        # è§„èŒƒåŒ–è¡¨æ ¼
        normalized_tables = []
        if tables:
            for t in tables:
                try:
                    df = pd.DataFrame(t)
                    if df.empty or df.shape[1] < 1:
                        continue
                    df = clean_cols(df)
                    # åˆ—é‡å‘½å
                    if colmap:
                        rename_map = {src: dst for src, dst in colmap.items() if src in df.columns}
                        if rename_map:
                            df = df.rename(columns=rename_map)
                    norm = coerce_to_standard(df, item_candidates, qty_candidates, price_candidates, store_name, id_candidates=id_candidates)
                    norm = norm.dropna(how="all")
                    norm = norm[norm["Item"].astype(str).str.strip() != ""]
                    if skip_total_rows:
                        norm = norm[~norm["Item"].astype(str).str.fullmatch(r"(?i)(total|åˆè®¡)")]
                    if not norm.empty:
                        normalized_tables.append(norm)
                except Exception:
                    continue

        # è‹¥ä»ä¸ºç©ºï¼Œå›é€€åˆ°é€è¡Œæ­£åˆ™
        if not normalized_tables and line_regex and text_concat:
            rows = []
            pat = re.compile(line_regex, re.X | re.I)
            for line in text_concat.splitlines():
                m = pat.match(line)
                if not m:
                    continue
                if active_vendor == "Foodstuffs_NI":
                    Line, ItemNo, Article, Item, Qty, UoM, UnitsPerUoM, UnitPrice, _Net, _TermFee, LineTotal = m.groups()
                    rows.append({
                        "Line": int(Line),
                        "ItemNo": ItemNo,
                        "Article": Article,
                        "Item": Item.strip(),
                        "Qty": float(Qty),
                        "UoM": UoM,
                        "UnitsPerUoM": float(UnitsPerUoM),
                        "Unit Price": float(UnitPrice),
                        "Line Total": float(LineTotal),
                        "Store": store_name
                    })
                elif active_vendor == "MyFoodBag":
                    ItemNo, Qty, Item, DeliveryDate, UnitPrice, LineTotal = m.groups()
                    rows.append({
                        "ItemNo": ItemNo,
                        "Item": Item.strip(),
                        "Qty": float(Qty.replace(",", "")),
                        "Delivery_Date": DeliveryDate,
                        "Unit Price": float(UnitPrice),
                        "Line Total": float(LineTotal.replace(",", "")),
                        "Store": store_name
                    })
                elif active_vendor == "WWNZ":
                    gd = m.groupdict()
                    rows.append({
                        "ItemNo": gd.get("itemno"),
                        "Item": gd.get("item").strip(),
                        "Qty": float(gd.get("ordqty")),
                        "Unit Price": float(gd.get("price")),
                        "Store": store_name
                    })
            if rows:
                normalized_tables.append(pd.DataFrame(rows))

        # åˆå¹¶ç»“æœ
        merged = pd.concat(normalized_tables, ignore_index=True) if normalized_tables else pd.DataFrame(columns=["Item","Qty","Unit Price","Store"])

        # æ ‡å‡†å­—æ®µè¡¥å……
        merged.insert(0, "Vendor", active_vendor or "Unknown")
        # æŠ¬å¤´å­—æ®µ
        po_num = header.get("PO_Number")
        ord_date = parse_date_safe(header.get("Order_Date"))
        deliv_date = parse_date_safe(header.get("Delivery_Date"))
        deliv_time = header.get("Delivery_Time")

        merged.insert(1, "PO_Number", po_num)
        merged.insert(2, "Order_Date", ord_date)
        merged.insert(3, "Delivery_Date", deliv_date)
        merged.insert(4, "Delivery_Time", deliv_time)
        # ç«™ç‚¹/é—¨åº—
        merged.insert(5, "SiteCode", site_code)

        # æ•°å€¼æ¸…æ´—
        if "Qty" in merged.columns:
            merged["Qty"] = normalize_numeric(merged["Qty"])
        if "Unit Price" in merged.columns:
            merged["Unit Price"] = normalize_numeric(merged["Unit Price"])
        if "Line Total" in merged.columns:
            merged["Line Total"] = normalize_numeric(merged["Line Total"])

        # å»é™¤è¡¨å¤´å™ªå£°
        merged = merged[~merged["Item"].astype(str).str.match(r"(?i)^(item|sku|code|description|å“å|åˆè®¡|total)$")]

        # æç¤ºè¯†åˆ«
        if active_vendor:
            st.caption(f"è¯†åˆ«åˆ°ä¾›åº”å•†ï¼š**{active_vendor}**ï¼ˆå¼•æ“ï¼š{engine_used} | é¡µç ï¼š{pages_used}ï¼‰")
        else:
            st.caption(f"æœªè¯†åˆ«ä¾›åº”å•†ï¼ˆä½¿ç”¨å½“å‰é€‰æ‹©/é»˜è®¤ï¼šå¼•æ“ {engine_used} | é¡µç  {pages_used}ï¼‰")

        return merged

# -----------------------
# ä¾§è¾¹æ è®¾ç½®
# -----------------------
with st.sidebar:
    st.header("è§£æè®¾ç½®")
    vendor_choice = st.selectbox("ä¾›åº”å•†", ["Auto"] + list(VENDOR_PROFILES.keys()), index=0)

    default_engine = "camelot-stream"
    default_pages = "all"
    default_store_re = r"(Store|é—¨åº—|åº—é¢)[:ï¼š]?\s*([A-Za-z0-9\- _]+)"
    if vendor_choice != "Auto":
        vcfg = VENDOR_PROFILES[vendor_choice]
        default_engine = vcfg.get("engine", default_engine)
        default_pages = vcfg.get("pages", default_pages)
        default_store_re = vcfg.get("store_regex", default_store_re)

    engine = st.selectbox("é¦–é€‰è§£æå¼•æ“", ["camelot-lattice", "camelot-stream", "tabula", "pdfplumber"],
                          index=["camelot-lattice","camelot-stream","tabula","pdfplumber"].index(default_engine))
    pages = st.text_input("é¡µç èŒƒå›´ï¼ˆå¦‚ 1 æˆ– 1-3 æˆ– allï¼‰", value=default_pages)
    enable_ocr = st.checkbox("æ‰«æä»¶å¯ç”¨ OCRï¼ˆè¾ƒæ…¢ï¼‰", value=False)

    item_cols_guess = st.text_input("å•†å“åˆ—å€™é€‰ï¼ˆé€—å·åˆ†éš”ï¼‰", "Item,Description,DESCRIPTION,Product Description,å“å")
    qty_cols_guess = st.text_input("æ•°é‡åˆ—å€™é€‰ï¼ˆé€—å·åˆ†éš”ï¼‰", "Qty,Quantity,QTY,æ•°é‡,Order Qty,ORD QTY")
    price_cols_guess = st.text_input("å•ä»·åˆ—å€™é€‰ï¼ˆé€—å·åˆ†éš”ï¼‰", "Unit Price,PRICE,Price,å•ä»·,Price Per Ord. Unit,PRICE EXCL")
    store_regex = st.text_input("åº—é¢/ä»“åº“è¯†åˆ«æ­£åˆ™ï¼ˆå¯è‡ªåŠ¨è¯†åˆ«SiteCode+Storeï¼‰", default_store_re)
    merge_to_one = st.checkbox("åˆå¹¶å¤šä¸ªæ–‡ä»¶ä¸ºä¸€ä¸ª CSV", value=True)

    st.subheader("é…ç½®ç®¡ç†")
    if st.button("å¯¼å‡ºä¾›åº”å•†é…ç½® JSON"):
        st.download_button("ä¸‹è½½ vendor_profiles.json",
                           data=json.dumps(VENDOR_PROFILES, ensure_ascii=False, indent=2),
                           file_name="vendor_profiles.json",
                           mime="application/json")
    uploaded_cfg = st.file_uploader("å¯¼å…¥/è¦†ç›–ä¾›åº”å•†é…ç½® JSON", type=["json"], accept_multiple_files=False, key="cfg_uploader")
    if uploaded_cfg:
        try:
            VENDOR_PROFILES.clear()
            VENDOR_PROFILES.update(json.loads(uploaded_cfg.read().decode("utf-8")))
            st.success("å·²å¯¼å…¥å¹¶åº”ç”¨æ–°çš„ä¾›åº”å•†é…ç½®ã€‚")
        except Exception as e:
            st.error(f"å¯¼å…¥å¤±è´¥ï¼š{e}")

    st.subheader("å¯¼å‡ºè®¾ç½®")
    export_format = st.selectbox(
        "å¯¼å‡ºæ¨¡ç‰ˆ",
        ["Auto by Vendor", "DCorder (å« price)", "CDDCorder (ä¸å« price)"],
        index=0
    )

    # é—¨åº—æ˜ å°„ï¼šsite_code æˆ– name â†’ store_id/name
    store_map_file = st.file_uploader("ä¸Šä¼ é—¨åº—æ˜ å°„ store_map.csvï¼ˆå¯é€‰ï¼‰", type=["csv"], key="store_map_uploader")
    store_map_df = None
    if store_map_file:
        try:
            store_map_df = pd.read_csv(store_map_file)
            store_map_df.columns = [c.strip().lower() for c in store_map_df.columns]
            st.success("å·²åŠ è½½é—¨åº—æ˜ å°„ï¼Œå°†ä¼˜å…ˆç”¨ site_code åŒ¹é…ï¼Œå…¶æ¬¡ç”¨ name æ¨¡ç³ŠåŒ¹é…ã€‚")
        except Exception as e:
            st.error(f"é—¨åº—æ˜ å°„è¯»å–å¤±è´¥ï¼š{e}")

uploaded = st.file_uploader("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª PO PDF", type=["pdf"], accept_multiple_files=True)

# -----------------------
# ä¸»æµç¨‹ï¼šè§£æ â†’ é—¨åº—æ˜ å°„ â†’ å¯¼å‡ºæ¨¡ç‰ˆ
# -----------------------
results = []
if uploaded:
    st.info("æç¤ºï¼šWWNZ/MyFoodBag å¤šä¸ºæ— è¾¹æ¡†æ–‡æœ¬åˆ—ï¼Œä¼˜å…ˆç”¨ camelot-streamï¼›Foodstuffs è‹¥è¡¨æ ¼å¸¦è¾¹æ¡†ï¼Œç”¨ camelot-latticeã€‚å¤±è´¥ä¼šè‡ªåŠ¨å›é€€åˆ°å…¶ä»–æ–¹å¼ã€‚")
    for f in uploaded:
        st.markdown(f"**ğŸ“„ {f.name}**")
        df = try_parse(
            pdf_bytes=f.read(),
            file_name=f.name,
            vendor_choice=vendor_choice,
            enable_ocr=enable_ocr,
            default_engine=engine,
            default_pages=pages,
            default_store_regex=store_regex,
            item_cols_guess=item_cols_guess,
            qty_cols_guess=qty_cols_guess,
            price_cols_guess=price_cols_guess
        )
        if df.empty:
            st.warning("æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¡Œï¼Œè¯·å°è¯•æ›´æ¢è§£æå¼•æ“æˆ–å¼€å¯ OCRã€‚")
        # â€”â€” é—¨åº—æ˜ å°„ï¼šè¡¥å…¨ store_id / name â€”â€”
        if not df.empty:
            store_ids, store_names = [], []
            for _, r in df.iterrows():
                sid, sname = map_store_to_ids(r.get("SiteCode"), r.get("Store"), store_map_df)
                store_ids.append(sid)
                store_names.append(sname)
            df["store_id"] = store_ids
            df["name"] = pd.Series(store_names).fillna(df.get("Store"))

        # â€”â€” é€‰æ‹©å¯¼å‡ºæ ¼å¼ï¼ˆè‡ªåŠ¨/æ‰‹åŠ¨ï¼‰ â€”â€”
        _fmt = export_format
        if _fmt == "Auto by Vendor":
            v0 = df["Vendor"].iloc[0] if not df.empty and "Vendor" in df.columns else None
            _fmt = "DCorder (å« price)" if choose_format_by_vendor(v0) == "DCorder" else "CDDCorder (ä¸å« price)"

        # â€”â€” ç”Ÿæˆå¯¼å…¥æ¨¡ç‰ˆæ•°æ®å¸§ â€”â€”
        if _fmt.startswith("DCorder"):
            export_df = to_DCorder_template(df)
        else:
            export_df = to_CDDCorder_template(df)

        # ç®€å•æ ¡éªŒï¼šæ ¸å¿ƒåˆ—ï¼ˆä»·æ ¼åˆ—ä»… DCorderï¼‰
        need_cols = ["store_id", "name", "sales_id", "order_date", "item_id", "quantity"]
        lack = [c for c in need_cols if c not in export_df.columns]
        if lack:
            st.warning(f"å½“å‰æ•°æ®ç¼ºå°‘å¿…è¦åˆ—ï¼š{lack}")

        # å±•ç¤ºå¯ç¼–è¾‘è¡¨æ ¼
        st.write("ğŸ‘‡ è¯·åœ¨ä¸‹æ–¹æ ¸å¯¹/è¡¥é½å¯¼å…¥æ¨¡ç‰ˆéœ€è¦çš„å­—æ®µï¼ˆå¯ç›´æ¥ç¼–è¾‘ï¼‰ï¼š")
        edited = st.data_editor(export_df, use_container_width=True, num_rows="dynamic", key=f"editor_{f.name}")

        # æ ¡éªŒæç¤º
        err_qty = edited["quantity"].isna().sum() if "quantity" in edited.columns else 0
        err_item = edited["item_id"].isna().sum() if "item_id" in edited.columns else 0
        st.write(f"æ ¡éªŒï¼šç¼ºå°‘ `item_id` è¡Œæ•° **{err_item}**ï¼›ç¼ºå°‘ `quantity` è¡Œæ•° **{err_qty}**ã€‚")
        if "price" in edited.columns:
            err_price = edited["price"].isna().sum()
            st.write(f"æ ¡éªŒï¼šç¼ºå°‘ `price` è¡Œæ•° **{err_price}**ã€‚")

        results.append((f.name, edited, _fmt))
    # â€”â€” å¯¼å‡ºæŒ‰é’® â€”â€”
    c1, c2 = st.columns(2)
    with c1:
        if results:
            for name, df_out, fmt_name in results:
                stub = Path(name).stem
                file_name = f"{stub}_{'DCorder' if 'DCorder' in fmt_name else 'CDDCorder'}.csv"
                csv_bytes = df_out.to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    label=f"â¬‡ï¸ ä¸‹è½½ {file_name}",
                    data=csv_bytes,
                    file_name=file_name,
                    mime="text/csv",
                    key=f"dl_{name}"
                )
    with c2:
        if results and merge_to_one:
            merged = pd.concat([df_out.assign(Source=name) for name, df_out, _fmt in results], ignore_index=True)
            csv_bytes = merged.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                label="â¬‡ï¸ ä¸‹è½½åˆå¹¶ CSVï¼ˆmerged_orders.csvï¼‰",
                data=csv_bytes,
                file_name="merged_orders.csv",
                mime="text/csv",
            )
else:
    st.info("è¯·åœ¨ä¸Šæ–¹ä¸Šä¼  PDF æ–‡ä»¶ã€‚")
