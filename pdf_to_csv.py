# app.py (robust v3)
# PO PDF â†’ CSV ç»Ÿä¸€è§£æä¸å¯¼å‡ºï¼ˆä¸‰å®¶ä¾›åº”å•† + åŒå¯¼å…¥æ¨¡ç‰ˆï¼‰
# æ›´æ–°è¦ç‚¹ï¼š
# 1) Foodstuffs çš„ item_id ä½¿ç”¨ **Article Number**ï¼ˆå­—æ®µå Articleï¼‰ï¼Œè€Œé Line Item #
# 2) My Food Bag æ–°å¢çº¯æ–‡æœ¬åå¤‡è§£æï¼šç”¨ pdftotext -layout æŠ½æ–‡æœ¬ â†’ ç”¨ 2+ ç©ºæ ¼åˆ‡åˆ—
# 3) ä¾›åº”å•†è¯†åˆ«å»æ‰ MFB çš„é€šç”¨å…³é”®è¯â€œPURCHASE ORDERâ€ï¼Œé¿å…æŠŠ WWNZ è¯¯åˆ¤æˆ MFB
# 4) å»¶ç»­å‰æ¬¡æ”¹åŠ¨ï¼šç¨³å¥æ•°å€¼æ¸…æ´— / Tabula stream / ItemNo å¹¿è°±è¯†åˆ«

import re
import json
import tempfile
import subprocess
from pathlib import Path
from datetime import datetime

import streamlit as st
import pandas as pd

st.set_page_config(page_title="PO PDF â†’ CSV Converter", layout="wide")
st.title("PO PDF â†’ CSV è½¬æ¢å™¨ï¼ˆä¸‰å®¶ä¾›åº”å•† & åŒå¯¼å…¥æ¨¡ç‰ˆï½œv3ï¼‰")

VENDOR_PROFILES = {
    "Foodstuffs_NI": {
        "detect_keywords": ["Foodstuffs North Island Limited", "Order Forecast", "O/F"],
        "engine": "camelot-lattice",
        "pages": "all",
        "store_regex": r"(?:Delivery\s+To|Delivery\s+Address)[:ï¼š]?\s*([^\n]+)",
        "column_map": {
            "Line": "Line",
            "Item #": "ItemNo",
            "Item#": "ItemNo",
            "Article Number": "Article",
            "Product Description": "Item",
            "Order Qty": "Qty",
            "Purchasing Unit of Measure": "UoM",
            "Units Per Purchasing UoM": "UnitsPerUoM",
            "Price Per Ord. Unit": "Unit Price",
            "Total Price": "Line Total"
        },
        "skip_total_rows": True,
        "header_extract": {
            "PO_Number": r"Order\s+Forecast\s+Number[:ï¼š]?\s*([0-9]+)",
            "Order_Date": r"Date\s+of\s+Order[:ï¼š]?\s*([0-9]{1,2}/[0-9]{1,2}/[0-9]{2,4})",
            "Delivery_Date": r"Delivery\s+Date[:ï¼š]?\s*([0-9]{1,2}/[0-9]{1,2}/[0-9]{2,4})"
        },
        "line_regex": r"""
          ^\s*(\d+)\s+                 # Line
          (\d+)\s+                     # Item #
          ([A-Za-z0-9\-]+)\s+          # Article
          (.+?)\s+                     # Product Description
          ([0-9,]+)\s+                 # Order Qty
          ([A-Z]{2,4})\s+              # UoM
          ([0-9,]+)\s+                 # Units Per UoM
          \$?([\d,]+\.\d{2})\s+        # Unit Price
          \$?([\d,]+\.\d{2})\s+        # Net (å¿½ç•¥)
          -?\$?([\d,]+\.\d{2})\s+      # Term Fee (å¿½ç•¥)
          \$?([\d,]+\.\d{2})\s*$       # Line Total
        """
    },
    "MyFoodBag": {
        # ç§»é™¤é€šç”¨â€œPURCHASE ORDERâ€å…³é”®å­—ï¼Œé¿å…è¯¯åˆ¤
        "detect_keywords": ["My Food Bag", "My Food Bag Limited", "GST Reg. No:"],
        "engine": "camelot-stream",
        "pages": "all",
        "store_regex": r"Delivery\s+Instructions[:ï¼š]?\s*([^\n]+)",
        "column_map": {
            "Item No.": "ItemNo",
            "ITEM NO.": "ItemNo",
            "Item No": "ItemNo",
            "QTY": "Qty",
            "DESCRIPTION": "Item",
            "Delivery Date": "Delivery_Date",
            "PRICE": "Unit Price",
            "TOTAL": "Line Total"
        },
        "skip_total_rows": True,
        "header_extract": {
            "PO_Number": r"Purchase\s+(?:Order\s+)?No[:ï¼š]?\s*([0-9]+)",
            "Order_Date": r"Order\s*Date[:ï¼š]?\s*([0-9]{1,2}/[0-9]{1,2}/[0-9]{2,4})"
        },
        "line_regex": r"""
          ^\s*([0-9]{6,})\s+           # Item No
          ([0-9,]+)\s+                 # QTY
          (.+?)\s+                     # DESCRIPTION
          ([0-9]{1,2}/[0-9]{1,2}/[0-9]{2,4})\s+  # Delivery Date
          \$?([\d,]+\.\d{2})\s+        # PRICE
          \$?([\d,]+\.\d{2})\s*$       # TOTAL
        """
    },
    "WWNZ": {
        "detect_keywords": [
            "WOOLWORTHS NZ", "WOOLWORTHS NEW ZEALAND", "PRODUCE ORDER NUMBER", "VENDOR COPY"
        ],
        "engine": "camelot-stream",
        "pages": "all",
        "store_regex": r"Deliver\s+To:\s*([0-9]{3,6})\s*\n\s*([^\n]+)",
        "column_map": {
            "ITEM DESCRIPTION": "Item",
            "ITEM NO": "ItemNo",
            "ITEM#": "ItemNo",
            "ORD QTY": "Qty",
            "PRICE EXCL": "Unit Price"
        },
        "skip_total_rows": True,
        "header_extract": {
            "PO_Number": r"PRODUCE\s+ORDER\s+NUMBER\s*:\s*([0-9]+)",
            "Order_Date": r"Order\s+Date\s*:\s*([0-9]{1,2}/[0-9]{1,2}/[0-9]{2,4})",
            "Delivery_Date": r"Delivery\s+Date\s*:\s*([0-9]{1,2}/[0-9]{1,2}/[0-9]{2,4})",
            "Delivery_Time": r"Delivery\s+Time\s*:\s*([0-9:\s]{4,})"
        },
        "line_regex": r"""
          ^\s*\d+\s+                    # è¡Œå·
          (?P<gtin>\d{8,14})\s+         # GTIN
          (?P<item>.+?)\s+              # æè¿°
          (?P<itemno>\d+)\s+            # ITEM NO
          (?P<om>[\d\.]+)\s+            # OM (å¯å¿½ç•¥)
          (?P<tihi>\d+x\d+)\s+          # TIxHI (å¯å¿½ç•¥)
          (?P<ordqty>[\d,]+)\s+         # ORD QTY
          (?P<taxes>\d+)\s+             # TAXES (å¯å¿½ç•¥)
          \$?(?P<price>[\d,]+\.\d{2})\s*$ # PRICE EXCL
        """
    }
}

# ---------- å·¥å…·å‡½æ•° ----------
def clean_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    for c in df.columns:
        if df[c].dtype == object:
            df[c] = df[c].astype(str).str.strip()
    return df

def normalize_numeric(series):
    s = pd.Series(series)
    s = s.apply(lambda x: (x[0] if isinstance(x, (list, tuple)) and len(x) > 0 else x))
    s = s.astype(str)
    s = s.str.replace(r"[\\s,\\$]", "", regex=True)
    s = s.str.extract(r"(-?\\d+(?:\\.\\d+)?)", expand=False)
    return pd.to_numeric(s, errors="coerce")

def pick_first_existing(df, candidates):
    for c in candidates or []:
        if c in df.columns:
            return c
    return None

def find_col_by_regex(df, pattern_list):
    for pat in pattern_list:
        for c in df.columns:
            if re.search(pat, str(c), re.I):
                return c
    return None

def parse_date_safe(s: str):
    if not s:
        return None
    for fmt in ("%d/%m/%Y", "%d/%m/%y", "%Y/%m/%d", "%d-%m-%Y", "%Y-%m-%d"):
        try:
            return datetime.strptime(s.strip(), fmt).date().isoformat()
        except Exception:
            continue
    return s

def extract_by_regex(pattern, text):
    if not pattern or not text:
        return None
    m = re.search(pattern, text, flags=re.IGNORECASE)
    if m:
        return m.group(1).strip() if m.lastindex else m.group(0).strip()
    return None

def extract_store(text, store_re):
    # æ”¯æŒ WWNZ (site_code + name) ä¸å•è¡Œ name
    m = re.search(store_re, text, flags=re.IGNORECASE)
    if not m:
        return None, None
    if m.lastindex and m.lastindex >= 2:
        a, b = m.group(1), m.group(2)
        if re.fullmatch(r"\\d{3,6}", a or ""):
            return a, b
        return None, f"{a} {b}".strip()
    val = m.group(1) if (m.lastindex and m.lastindex >= 1) else m.group(0)
    if re.fullmatch(r"\\d{3,6}", val or ""):
        return val, None
    return None, val

def detect_vendor(text_concat: str) -> str | None:
    text_upper = (text_concat or "").upper()
    # å…ˆåŒ¹é… WWNZï¼Œé¿å…è¢«å…¶å®ƒé€šç”¨è¯ç›–æ‰
    if any(kw.upper() in text_upper for kw in VENDOR_PROFILES["WWNZ"]["detect_keywords"]):
        return "WWNZ"
    for vendor in ("Foodstuffs_NI","MyFoodBag"):
        cfg = VENDOR_PROFILES[vendor]
        for kw in cfg.get("detect_keywords", []):
            if kw.upper() in text_upper:
                return vendor
    return None

# ---------- è§£æå¼•æ“ ----------
def parse_with_pdfplumber(pdf_path, pages_spec):
    import pdfplumber
    dfs, raw_texts = [], []
    with pdfplumber.open(pdf_path) as pdf:
        idx = range(len(pdf.pages))
        if pages_spec != "all":
            wanted = set()
            for seg in pages_spec.split(","):
                seg = seg.strip()
                if "-" in seg:
                    a, b = seg.split("-")
                    wanted |= set(range(int(a)-1, int(b)))
                else:
                    wanted.add(int(seg)-1)
            idx = [i for i in idx if i in wanted]
        for i in idx:
            page = pdf.pages[i]
            raw_texts.append(page.extract_text() or "")
            # extract_tablesï¼ˆå¤šè¡¨ï¼‰
            tables = []
            try:
                tables = page.extract_tables() or []
            except Exception:
                tbl = page.extract_table()
                if tbl and len(tbl) > 1:
                    tables = [tbl]
            for table in tables:
                if table and len(table) > 1:
                    df = pd.DataFrame(table[1:], columns=table[0])
                    dfs.append(df)
    return dfs, "\\n".join(raw_texts)

def parse_with_camelot(pdf_path, pages_spec, flavor):
    import camelot
    pages_arg = pages_spec if pages_spec != "all" else "all"
    tables = camelot.read_pdf(pdf_path, pages=pages_arg, flavor=flavor, strip_text="\\n")
    dfs = []
    for t in tables:
        df = t.df
        if df.shape[0] > 1:
            new_df = df.copy()
            new_df.columns = new_df.iloc[0]
            new_df = new_df.iloc[1:].reset_index(drop=True)
            dfs.append(new_df)
    return dfs

def parse_with_tabula(pdf_path, pages_spec):
    from tabula import read_pdf
    pages_arg = pages_spec if pages_spec != "all" else "all"
    dfs = read_pdf(pdf_path, pages=pages_arg, multiple_tables=True, lattice=False, stream=True, guess=True) or []
    return dfs

def ocr_pages_to_text(pdf_path):
    from pdf2image import convert_from_path
    import pytesseract
    images = convert_from_path(pdf_path, dpi=200)
    texts = []
    for img in images:
        texts.append(pytesseract.image_to_string(img))
    return "\\n".join(texts)

def pdftotext_layout(pdf_path):
    try:
        out = subprocess.check_output(["pdftotext", "-layout", pdf_path, "-"], stderr=subprocess.STDOUT)
        return out.decode("utf-8", errors="ignore")
    except Exception:
        return ""

# ---------- è§„èŒƒåŒ– ----------
def coerce_to_standard(df, item_candidates, qty_candidates, price_candidates, fallback_store, id_candidates=None):
    df = clean_cols(df)

    item_col = pick_first_existing(df, item_candidates) or find_col_by_regex(df, [r"\\bdesc(ription)?\\b", r"product", r"å“å"]) or df.columns[0]
    qty_col = pick_first_existing(df, qty_candidates) or find_col_by_regex(df, [r"\\bqty\\b", r"quantity", r"è®¢å•?æ•°é‡", r"ord\\s*qty"])
    price_col = pick_first_existing(df, price_candidates) or find_col_by_regex(df, [r"unit\\s*price|price\\s*excl|ä»·æ ¼|å•ä»·"])

    out = pd.DataFrame()
    out["Item"] = df[item_col].astype(str)
    out["Qty"] = normalize_numeric(df[qty_col]) if (qty_col and qty_col in df.columns) else pd.NA
    out["Unit Price"] = normalize_numeric(df[price_col]) if (price_col and price_col in df.columns) else pd.NA

    if id_candidates is None:
        id_candidates = ["ItemNo","Item #","Item#","Item No.","ITEM NO","Article Number","Article","SKU","PLU","GTIN","UPC","Barcode"]
    id_col = pick_first_existing(df, id_candidates) or find_col_by_regex(df, [r"item\\s*#?", r"item\\s*no", r"article", r"sku", r"gtin|upc|plu|barcode"])
    out["ItemNo"] = df[id_col].astype(str) if id_col and id_col in df.columns else pd.NA

    # ä¿ç•™ Articleï¼ˆFoodstuffs çš„ Article Numberï¼‰
    if "Article" in df.columns:
        out["Article"] = df["Article"]

    out["Store"] = fallback_store
    return out

# ---------- å¯¼å‡ºæ¨¡ç‰ˆ ----------
def choose_item_id(df: pd.DataFrame) -> pd.Series:
    """Foodstuffs ç”¨ Articleï¼ˆArticle Numberï¼‰ï¼Œå…¶å®ƒç”¨ ItemNoï¼›è‹¥ä¸ºç©ºå†å…œåº•."""
    vendor = (df.get("Vendor").iloc[0] if "Vendor" in df.columns and not df.empty else "").upper()
    if vendor == "FOODSTUFFS_NI":
        col = "Article" if "Article" in df.columns else "ItemNo"
        s = df.get(col)
        if s is None or s.isna().all():
            s = df.get("ItemNo")
        return s
    # é»˜è®¤ï¼šItemNo
    return df.get("ItemNo")

def map_store_to_ids(site_code, store_name, store_map_df):
    if store_map_df is None:
        return None, None
    df = store_map_df.copy()
    df.columns = [c.strip().lower() for c in df.columns]
    if site_code:
        m = df[df.get("site_code", "").astype(str).str.strip() == str(site_code).strip()]
        if not m.empty:
            return m.iloc[0].get("store_id"), m.iloc[0].get("name")
    if store_name:
        key = str(store_name).strip().lower()
        m = df[df.get("name", "").astype(str).str.lower().str.contains(re.escape(key))]
        if not m.empty:
            return m.iloc[0].get("store_id"), m.iloc[0].get("name")
    return None, None

def to_DCorder_template(df):
    return pd.DataFrame({
        "store_id": df.get("store_id"),
        "name": df.get("name"),
        "sales_id": df.get("PO_Number"),
        "order_date": df.get("Delivery_Date").fillna(df.get("Order_Date")),
        "item_id": choose_item_id(df),
        "quantity": df.get("Qty"),
        "price": df.get("Unit Price")
    })

def to_CDDCorder_template(df):
    return pd.DataFrame({
        "store_id": df.get("store_id"),
        "name": df.get("name"),
        "sales_id": df.get("PO_Number"),
        "order_date": df.get("Delivery_Date").fillna(df.get("Order_Date")),
        "item_id": choose_item_id(df),
        "quantity": df.get("Qty")
    })

def choose_format_by_vendor(vendor: str) -> str:
    if (vendor or "").upper() == "WWNZ":
        return "CDDCorder"
    return "DCorder"

# ---------- ç‰¹æ®Šï¼šMFB çº¯æ–‡æœ¬åå¤‡è§£æ ----------
def parse_mfb_from_text(text: str, store_name_hint=None):
    """
    ç”¨ pdftotext -layout çš„åŸæ–‡è¿›è¡Œé€è¡Œè§£æï¼š
    è§„åˆ™ï¼šæ‰¾åŒ…å« 'Item' & 'Description' çš„è¡¨å¤´è¡Œï¼›
    ä¹‹åæ¯è¡Œç”¨ 2+ ç©ºæ ¼åˆ‡åˆ†ï¼›æœ€åä¸‰æ®µè§†ä½œ DeliveryDate, Price, Totalï¼›ç¬¬ä¸€æ®µ ItemNoï¼Œç¬¬äºŒæ®µ Qtyï¼Œä¸­é—´æ‹¼ä¸ºæè¿°ã€‚
    """
    if not text:
        return None
    lines = [ln.rstrip() for ln in text.splitlines() if ln.strip()]
    # æ‰¾åˆ°è¡¨å¤´ç´¢å¼•
    hdr_idx = -1
    for i, ln in enumerate(lines):
        if re.search(r"item\\s*no", ln, re.I) and re.search(r"desc", ln, re.I):
            hdr_idx = i
            break
    if hdr_idx == -1:
        return None
    rows = []
    for ln in lines[hdr_idx+1:]:
        if re.search(r"\\btotal\\b", ln, re.I):
            # æ€»è®¡è¡Œåˆ°æ­¤ä¸ºæ­¢
            break
        parts = re.split(r"\\s{2,}", ln.strip())
        if len(parts) < 4:
            continue
        # æœŸæœ›ï¼šitemno, qty, desc..., delivery, price, total(å¯é€‰)
        if len(parts) >= 6:
            itemno = parts[0]
            qty = parts[1]
            delivery = parts[-3]
            price = parts[-2]
            total = parts[-1]
            desc = " ".join(parts[2:-3])
        elif len(parts) == 5:
            itemno = parts[0]
            qty = parts[1]
            delivery = parts[-3]
            price = parts[-2]
            total = parts[-1]
            desc = " ".join(parts[2:-3])
        else:
            # ç®€åŒ–å…œåº•ï¼šitemno, qty, desc, price/total æ··åˆ
            itemno = parts[0]
            qty = parts[1]
            delivery = None
            price = parts[-1]
            total = None
            desc = " ".join(parts[2:-1])
        rows.append({
            "ItemNo": itemno,
            "Item": desc.strip(),
            "Qty": qty,
            "Delivery_Date": delivery,
            "Unit Price": price,
            "Line Total": total,
            "Store": store_name_hint or "My Food Bag"
        })
    if not rows:
        return None
    df = pd.DataFrame(rows)
    # æ•°å€¼ä¸æ—¥æœŸæ¸…æ´—
    df["Qty"] = normalize_numeric(df["Qty"])
    if "Unit Price" in df:
        df["Unit Price"] = normalize_numeric(df["Unit Price"])
    return df

# ---------- ä¸»è§£æ ----------
def try_parse(pdf_bytes, file_name, vendor_choice, enable_ocr, default_engine, default_pages, default_store_regex,
              item_cols_guess, qty_cols_guess, price_cols_guess):
    with tempfile.TemporaryDirectory() as td:
        fp = Path(td) / file_name
        fp.write_bytes(pdf_bytes)

        # æ–‡æœ¬æå–ï¼špdfplumber â†’ pdftotext â†’ OCR
        text_concat = ""
        try:
            import pdfplumber
            with pdfplumber.open(str(fp)) as pdf:
                text_concat = "\\n".join([p.extract_text() or "" for p in pdf.pages])
        except Exception:
            text_concat = ""
        if not (text_concat or "").strip():
            text_concat = pdftotext_layout(str(fp))
        if enable_ocr and not (text_concat or "").strip():
            try:
                text_concat = ocr_pages_to_text(str(fp))
            except Exception:
                pass

        # ä¾›åº”å•†é…ç½®
        active_vendor = detect_vendor(text_concat) if vendor_choice == "Auto" else (vendor_choice if vendor_choice in VENDOR_PROFILES else None)
        profile = VENDOR_PROFILES.get(active_vendor, {}) if active_vendor else {}
        engine_used = profile.get("engine", default_engine)
        pages_used = profile.get("pages", default_pages)
        store_re_used = profile.get("store_regex", default_store_regex)
        colmap = profile.get("column_map", {})
        skip_total_rows = profile.get("skip_total_rows", True)
        header_extract = profile.get("header_extract", {})
        line_regex = profile.get("line_regex", "")

        # æŠ¬å¤´å­—æ®µ
        header = {}
        for k, pat in header_extract.items():
            v = extract_by_regex(pat, text_concat)
            header[k] = v

        # åº—é¢/ä»“
        site_code, store_name = extract_store(text_concat, store_re_used) if text_concat else (None, None)

        # è¡¨æ ¼è§£æï¼šCamelot â†’ Tabula(stream) â†’ pdfplumber
        tables = []
        try:
            if engine_used.startswith("camelot"):
                flavor = "lattice" if engine_used.endswith("lattice") else "stream"
                tables = parse_with_camelot(str(fp), pages_used, flavor)
        except Exception as e:
            st.warning(f"[{file_name}] Camelot è§£æå¤±è´¥ï¼š{e}")
        if not tables:
            try:
                tables = parse_with_tabula(str(fp), pages_used)
            except Exception:
                pass
        if not tables:
            try:
                tables, text2 = parse_with_pdfplumber(str(fp), pages_used)
                if text2 and (not text_concat):
                    text_concat = text2
            except Exception:
                pass

        # æ„å»ºåˆ—å€™é€‰
        item_candidates = [k for k, v in colmap.items() if v == "Item"] or [c.strip() for c in item_cols_guess.split(",")]
        qty_candidates = [k for k, v in colmap.items() if v == "Qty"] or [c.strip() for c in qty_cols_guess.split(",")]
        price_candidates = [k for k, v in colmap.items() if v == "Unit Price"] or [c.strip() for c in price_cols_guess.split(",")]
        id_candidates = [k for k, v in colmap.items() if v in ("ItemNo","ITEM NO")] or ["ItemNo","Item #","Item#","Item No.","ITEM NO","Article Number","Article","SKU","PLU","GTIN","UPC","Barcode"]

        # è§„èŒƒåŒ–
        normalized_tables = []
        if tables:
            for t in tables:
                try:
                    df = pd.DataFrame(t)
                    if df.empty or df.shape[1] < 1:
                        continue
                    df = clean_cols(df)
                    # åˆ—é‡å‘½å
                    if colmap:
                        rename_map = {src: dst for src, dst in colmap.items() if src in df.columns}
                        if rename_map:
                            df = df.rename(columns=rename_map)
                    norm = coerce_to_standard(df, item_candidates, qty_candidates, price_candidates, store_name, id_candidates=id_candidates)
                    norm = norm.dropna(how="all")
                    norm = norm[norm["Item"].astype(str).str.strip() != ""]
                    if skip_total_rows:
                        norm = norm[~norm["Item"].astype(str).str.fullmatch(r"(?i)(total|åˆè®¡)")]
                    if not norm.empty:
                        normalized_tables.append(norm)
                except Exception:
                    continue

        # é€è¡Œæ­£åˆ™å›é€€ / MFB ä¸“ç”¨æ–‡æœ¬å›é€€
        if not normalized_tables and text_concat:
            if active_vendor == "MyFoodBag":
                mfb_df = parse_mfb_from_text(text_concat, store_name_hint=store_name or "My Food Bag")
                if mfb_df is not None and not mfb_df.empty:
                    normalized_tables.append(mfb_df)
            if not normalized_tables and line_regex:
                rows = []
                pat = re.compile(line_regex, re.X | re.I)
                for line in text_concat.splitlines():
                    m = pat.match(line)
                    if not m:
                        continue
                    if active_vendor == "Foodstuffs_NI":
                        Line, ItemNo, Article, Item, Qty, UoM, UnitsPerUoM, UnitPrice, _Net, _TermFee, LineTotal = m.groups()
                        rows.append({
                            "Line": int(Line),
                            "ItemNo": ItemNo,
                            "Article": Article,
                            "Item": Item.strip(),
                            "Qty": float(Qty.replace(",", "")),
                            "UoM": UoM,
                            "UnitsPerUoM": float(UnitsPerUoM.replace(",", "")),
                            "Unit Price": float(UnitPrice.replace(",", "")),
                            "Line Total": float(LineTotal.replace(",", "")),
                            "Store": store_name
                        })
                    elif active_vendor == "MyFoodBag":
                        ItemNo, Qty, Item, DeliveryDate, UnitPrice, LineTotal = m.groups()
                        rows.append({
                            "ItemNo": ItemNo,
                            "Item": Item.strip(),
                            "Qty": float(Qty.replace(",", "")),
                            "Delivery_Date": DeliveryDate,
                            "Unit Price": float(UnitPrice.replace(",", "")),
                            "Line Total": float(LineTotal.replace(",", "")),
                            "Store": store_name or "My Food Bag"
                        })
                    elif active_vendor == "WWNZ":
                        gd = m.groupdict()
                        rows.append({
                            "ItemNo": gd.get("itemno"),
                            "Item": gd.get("item").strip(),
                            "Qty": float(gd.get("ordqty").replace(",", "")),
                            "Unit Price": float(gd.get("price").replace(",", "")),
                            "Store": store_name
                        })
                if rows:
                    normalized_tables.append(pd.DataFrame(rows))

        merged = pd.concat(normalized_tables, ignore_index=True) if normalized_tables else pd.DataFrame(columns=["Item","Qty","Unit Price","Store"])

        # æ ‡å‡†å­—æ®µ
        merged.insert(0, "Vendor", active_vendor or "Unknown")
        merged.insert(1, "PO_Number", header.get("PO_Number"))
        merged.insert(2, "Order_Date", parse_date_safe(header.get("Order_Date")))
        merged.insert(3, "Delivery_Date", parse_date_safe(header.get("Delivery_Date")))
        merged.insert(4, "Delivery_Time", header.get("Delivery_Time"))
        merged.insert(5, "SiteCode", site_code)

        # æ•°å€¼æ¸…æ´—
        if "Qty" in merged.columns:
            merged["Qty"] = normalize_numeric(merged["Qty"])
        if "Unit Price" in merged.columns:
            merged["Unit Price"] = normalize_numeric(merged["Unit Price"])
        if "Line Total" in merged.columns:
            merged["Line Total"] = normalize_numeric(merged["Line Total"])

        if "Item" in merged.columns:
            merged = merged[~merged["Item"].astype(str).str.match(r"(?i)^(item|sku|code|description|å“å|åˆè®¡|total)$")]

        # è¯†åˆ«æç¤º
        if active_vendor:
            st.caption(f"è¯†åˆ«åˆ°ä¾›åº”å•†ï¼š**{active_vendor}**ï¼ˆå¼•æ“ï¼š{engine_used} | é¡µç ï¼š{pages_used}ï¼‰")
        else:
            st.caption(f"æœªè¯†åˆ«ä¾›åº”å•†ï¼ˆä½¿ç”¨å½“å‰é€‰æ‹©/é»˜è®¤ï¼šå¼•æ“ {engine_used} | é¡µç  {pages_used}ï¼‰")

        return merged

# ---------- ä¾§è¾¹æ  ----------
with st.sidebar:
    st.header("è§£æè®¾ç½®")
    vendor_choice = st.selectbox("ä¾›åº”å•†", ["Auto"] + list(VENDOR_PROFILES.keys()), index=0)

    default_engine = "camelot-stream"
    default_pages = "all"
    default_store_re = r"(Store|é—¨åº—|åº—é¢)[:ï¼š]?\s*([A-Za-z0-9\- _]+)"
    if vendor_choice != "Auto":
        vcfg = VENDOR_PROFILES[vendor_choice]
        default_engine = vcfg.get("engine", default_engine)
        default_pages = vcfg.get("pages", default_pages)
        default_store_re = vcfg.get("store_regex", default_store_re)

    engine = st.selectbox("é¦–é€‰è§£æå¼•æ“", ["camelot-lattice", "camelot-stream", "tabula", "pdfplumber"],
                          index=["camelot-lattice","camelot-stream","tabula","pdfplumber"].index(default_engine))
    pages = st.text_input("é¡µç èŒƒå›´ï¼ˆå¦‚ 1 æˆ– 1-3 æˆ– allï¼‰", value=default_pages)
    enable_ocr = st.checkbox("æ‰«æä»¶å¯ç”¨ OCRï¼ˆè¾ƒæ…¢ï¼‰", value=False)

    item_cols_guess = st.text_input("å•†å“åˆ—å€™é€‰ï¼ˆé€—å·åˆ†éš”ï¼‰", "Item,Description,DESCRIPTION,Product Description,å“å")
    qty_cols_guess = st.text_input("æ•°é‡åˆ—å€™é€‰ï¼ˆé€—å·åˆ†éš”ï¼‰", "Qty,Quantity,QTY,æ•°é‡,Order Qty,ORD QTY,ORD QTY.")
    price_cols_guess = st.text_input("å•ä»·åˆ—å€™é€‰ï¼ˆé€—å·åˆ†éš”ï¼‰", "Unit Price,PRICE,Price,å•ä»·,Price Per Ord. Unit,PRICE EXCL")
    store_regex = st.text_input("åº—é¢/ä»“åº“è¯†åˆ«æ­£åˆ™ï¼ˆå¯è‡ªåŠ¨è¯†åˆ«SiteCode+Storeï¼‰", default_store_re)
    merge_to_one = st.checkbox("åˆå¹¶å¤šä¸ªæ–‡ä»¶ä¸ºä¸€ä¸ª CSV", value=True)

    st.subheader("å¯¼å‡ºè®¾ç½®")
    export_format = st.selectbox("å¯¼å‡ºæ¨¡ç‰ˆ", ["Auto by Vendor", "DCorder (å« price)", "CDDCorder (ä¸å« price)"], index=0)
    store_map_file = st.file_uploader("ä¸Šä¼ é—¨åº—æ˜ å°„ store_map.csvï¼ˆå¯é€‰ï¼‰", type=["csv"], key="store_map_uploader")
    store_map_df = None
    if store_map_file:
        try:
            store_map_df = pd.read_csv(store_map_file)
            store_map_df.columns = [c.strip().lower() for c in store_map_df.columns]
            st.success("å·²åŠ è½½é—¨åº—æ˜ å°„ï¼Œå°†ä¼˜å…ˆç”¨ site_code åŒ¹é…ï¼Œå…¶æ¬¡ç”¨ name æ¨¡ç³ŠåŒ¹é…ã€‚")
        except Exception as e:
            st.error(f"é—¨åº—æ˜ å°„è¯»å–å¤±è´¥ï¼š{e}")

uploaded = st.file_uploader("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª PO PDF", type=["pdf"], accept_multiple_files=True)

# ---------- ä¸»æµç¨‹ ----------
results = []
if uploaded:
    st.info("æç¤ºï¼šWWNZ/My Food Bag å¤šä¸ºæ— è¾¹æ¡†æ–‡æœ¬åˆ—ï¼Œä¼˜å…ˆç”¨ camelot-streamï¼›Foodstuffs è‹¥è¡¨æ ¼å¸¦è¾¹æ¡†ï¼Œç”¨ camelot-latticeã€‚å¤±è´¥ä¼šè‡ªåŠ¨å›é€€åˆ°å…¶ä»–æ–¹å¼ã€‚")
    for f in uploaded:
        st.markdown(f"**ğŸ“„ {f.name}**")
        df = try_parse(
            pdf_bytes=f.read(),
            file_name=f.name,
            vendor_choice=vendor_choice,
            enable_ocr=enable_ocr,
            default_engine=engine,
            default_pages=pages,
            default_store_regex=store_regex,
            item_cols_guess=item_cols_guess,
            qty_cols_guess=qty_cols_guess,
            price_cols_guess=price_cols_guess
        )
        if df.empty:
            st.warning("æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¡Œï¼Œè¯·å°è¯•æ›´æ¢è§£æå¼•æ“æˆ–å¼€å¯ OCRã€‚")

        # â€”â€” é—¨åº—æ˜ å°„ï¼šè¡¥å…¨ store_id / name â€”â€”
        if not df.empty:
            store_ids, store_names = [], []
            for _, r in df.iterrows():
                sid, sname = map_store_to_ids(r.get("SiteCode"), r.get("Store"), store_map_df)
                store_ids.append(sid)
                store_names.append(sname)
            df["store_id"] = store_ids
            base_name = df.get("Store")
            df["name"] = pd.Series(store_names).fillna(base_name)
            if (df["name"].isna().all() or (df["name"].astype(str) == "None").all()) and (df.get("Vendor").astype(str).str.contains("MyFoodBag").any()):
                df["name"] = "My Food Bag"

        # â€”â€” é€‰æ‹©å¯¼å‡ºæ ¼å¼ â€”â€”
        _fmt = export_format
        if _fmt == "Auto by Vendor":
            v0 = df["Vendor"].iloc[0] if not df.empty and "Vendor" in df.columns else None
            _fmt = "DCorder (å« price)" if choose_format_by_vendor(v0) == "DCorder" else "CDDCorder (ä¸å« price)"

        # â€”â€” ç”Ÿæˆå¯¼å…¥æ¨¡ç‰ˆ â€”â€”
        export_df = to_DCorder_template(df) if _fmt.startswith("DCorder") else to_CDDCorder_template(df)

        # æ ¡éªŒæç¤º
        need_cols = ["store_id", "name", "sales_id", "order_date", "item_id", "quantity"]
        lack = [c for c in need_cols if c not in export_df.columns]
        if lack:
            st.warning(f"å½“å‰æ•°æ®ç¼ºå°‘å¿…è¦åˆ—ï¼š{lack}")

        st.write("ğŸ‘‡ è¯·åœ¨ä¸‹æ–¹æ ¸å¯¹/è¡¥é½å¯¼å…¥æ¨¡ç‰ˆéœ€è¦çš„å­—æ®µï¼ˆå¯ç›´æ¥ç¼–è¾‘ï¼‰ï¼š")
        edited = st.data_editor(export_df, use_container_width=True, num_rows="dynamic", key=f"editor_{f.name}")

        err_qty = edited["quantity"].isna().sum() if "quantity" in edited.columns else 0
        err_item = edited["item_id"].isna().sum() if "item_id" in edited.columns else 0
        st.write(f"æ ¡éªŒï¼šç¼ºå°‘ `item_id` è¡Œæ•° **{err_item}**ï¼›ç¼ºå°‘ `quantity` è¡Œæ•° **{err_qty}**ã€‚")
        if "price" in edited.columns:
            err_price = edited["price"].isna().sum()
            st.write(f"æ ¡éªŒï¼šç¼ºå°‘ `price` è¡Œæ•° **{err_price}**ã€‚")

        results.append((f.name, edited, _fmt))

    # â€”â€” å¯¼å‡º â€”â€”
    c1, c2 = st.columns(2)
    with c1:
        if results:
            for name, df_out, fmt_name in results:
                stub = Path(name).stem
                file_name = f"{stub}_{'DCorder' if 'DCorder' in fmt_name else 'CDDCorder'}.csv"
                csv_bytes = df_out.to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    label=f"â¬‡ï¸ ä¸‹è½½ {file_name}",
                    data=csv_bytes,
                    file_name=file_name,
                    mime="text/csv",
                    key=f"dl_{name}"
                )
    with c2:
        if results and merge_to_one:
            merged = pd.concat([df_out.assign(Source=name) for name, df_out, _fmt in results], ignore_index=True)
            csv_bytes = merged.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                label="â¬‡ï¸ ä¸‹è½½åˆå¹¶ CSVï¼ˆmerged_orders.csvï¼‰",
                data=csv_bytes,
                file_name="merged_orders.csv",
                mime="text/csv",
            )
else:
    st.info("è¯·åœ¨ä¸Šæ–¹ä¸Šä¼  PDF æ–‡ä»¶ã€‚")
